{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Speed Dating: Who to Date Long Term</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What influences love at first sight? (Or, at least, love in the first four minutes?) This dataset was compiled by Columbia Business School professors Ray Fisman and Sheena Iyengar for their paper Gender Differences in Mate Selection: Evidence From a Speed Dating Experiment.<br>\n",
    "\n",
    "Data was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four minute \"first date\" with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes: Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests.<br>\n",
    "\n",
    "The dataset also includes questionnaire data gathered from participants at different points in the process. These fields include: demographics, dating habits, self-perception across key attributes, beliefs on what others find valuable in a mate, and lifestyle information. See the Speed Dating Data Key document below for details.<br>\n",
    "\n",
    "For more analysis from Iyengar and Fisman, read Racial Preferences in Dating.<br>\n",
    "\n",
    "Data Exploration Ideas<br>\n",
    "\n",
    "What are the least desirable attributes in a male partner? Does this differ for female partners?<br>\n",
    "How important do people think attractiveness is in potential mate selection vs. its real impact?<br>\n",
    "Are shared interests more important than a shared racial background?<br>\n",
    "Can people accurately predict their own perceived value in the dating market?<br>\n",
    "In terms of getting a second date, is it better to be someone's first speed date of the night or their last?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version is 0.18.0.\n",
      "numpy version is 1.10.4.\n",
      "scikit-learn version is 0.17.1.\n",
      "seaborn version is 0.7.1.\n",
      "matplotlib version is 1.5.1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "print('pandas version is {}.'.format(pd.__version__))\n",
    "print('numpy version is {}.'.format(np.__version__))\n",
    "print('scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('seaborn version is {}.'.format(sns.__version__))\n",
    "print('matplotlib version is {}.'.format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This set has 8378 data points and 195 features.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Speed Dating Data.csv\")\n",
    "print \"This set has {} data points and {} features.\".format(*data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Data Exploration</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Samples for each Feature</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wave 8378 ||| samerace 8378 ||| round 8378 ||| position 8378 ||| partner 8378 ||| order 8378 ||| match 8378 ||| iid 8378 ||| idg 8378 ||| gender 8378 ||| dec_o 8378 ||| dec 8378 ||| condtn 8378 ||| id 8377 ||| pid 8368 ||| race 8315 ||| field 8315 ||| race_o 8305 ||| yoga 8299 ||| tvsports 8299 ||| tv 8299 ||| theater 8299 ||| sports 8299 ||| sinc2_1 8299 ||| sinc1_1 8299 ||| shopping 8299 ||| reading 8299 ||| music 8299 ||| museums 8299 ||| movies 8299 ||| intel2_1 8299 ||| intel1_1 8299 ||| imprelig 8299 ||| imprace 8299 ||| hiking 8299 ||| goal 8299 ||| go_out 8299 ||| gaming 8299 ||| fun2_1 8299 ||| from 8299 ||| exercise 8299 ||| dining 8299 ||| concerts 8299 ||| clubbing 8299 ||| attr2_1 8299 ||| attr1_1 8299 ||| art 8299 ||| field_cd 8296 ||| shar2_1 8289 ||| pf_o_sin 8289 ||| pf_o_int 8289 ||| pf_o_att 8289 ||| fun1_1 8289 ||| career 8289 ||| amb2_1 8289 ||| age 8283 ||| date 8281 ||| pf_o_fun 8280 ||| amb1_1 8279 ||| exphappy 8277 ||| age_o 8274 ||| sinc3_1 8273 ||| intel3_1 8273 ||| fun3_1 8273 ||| attr3_1 8273 ||| amb3_1 8273 ||| pf_o_amb 8271 ||| shar1_1 8257 ||| pf_o_sha 8249 ||| career_c 8240 ||| int_corr 8220 ||| attr 8176 ||| attr_o 8166 ||| like 8138 ||| like_o 8128 ||| sinc 8101 ||| sinc_o 8091 ||| intel 8082 ||| intel_o 8072 ||| prob 8069 ||| prob_o 8060 ||| fun 8028 ||| fun_o 8018 ||| met 8003 ||| met_o 7993 ||| amb 7666 ||| amb_o 7656 ||| sinc3_2 7463 ||| sinc1_2 7463 ||| shar1_2 7463 ||| satis_2 7463 ||| length 7463 ||| intel3_2 7463 ||| intel1_2 7463 ||| fun3_2 7463 ||| fun1_2 7463 ||| attr3_2 7463 ||| amb3_2 7463 ||| amb1_2 7463 ||| attr1_2 7445 ||| numdat_2 7433 ||| zipcode 7314 ||| shar 7311 ||| shar_o 7302 ||| match_es 7205 ||| positin1 6532 ||| sinc4_1 6489 ||| intel4_1 6489 ||| fun4_1 6489 ||| attr4_1 6489 ||| amb4_1 6489 ||| shar4_1 6467 ||| sinc4_2 5775 ||| sinc2_2 5775 ||| shar4_2 5775 ||| shar2_2 5775 ||| intel4_2 5775 ||| intel2_2 5775 ||| fun4_2 5775 ||| fun2_2 5775 ||| attr4_2 5775 ||| attr2_2 5775 ||| amb4_2 5775 ||| amb2_2 5775 ||| undergra 4914 ||| sinc5_1 4906 ||| intel5_1 4906 ||| fun5_1 4906 ||| attr5_1 4906 ||| amb5_1 4906 ||| sinc5_2 4377 ||| intel5_2 4377 ||| fun5_2 4377 ||| attr5_2 4377 ||| amb5_2 4377 ||| income 4279 ||| sinc1_s 4096 ||| shar1_s 4096 ||| intel1_s 4096 ||| fun1_s 4096 ||| attr1_s 4096 ||| amb1_s 4096 ||| sinc3_s 4000 ||| intel3_s 4000 ||| fun3_s 4000 ||| attr3_s 4000 ||| amb3_s 4000 ||| you_call 3974 ||| them_cal 3974 ||| sinc3_3 3974 ||| sinc1_3 3974 ||| shar1_3 3974 ||| intel3_3 3974 ||| intel1_3 3974 ||| fun3_3 3974 ||| fun1_3 3974 ||| date_3 3974 ||| attr3_3 3974 ||| attr1_3 3974 ||| amb3_3 3974 ||| amb1_3 3974 ||| tuition 3583 ||| mn_sat 3133 ||| sinc4_3 2959 ||| sinc2_3 2959 ||| shar4_3 2959 ||| intel4_3 2959 ||| intel2_3 2959 ||| fun4_3 2959 ||| fun2_3 2959 ||| attr4_3 2959 ||| attr2_3 2959 ||| amb4_3 2959 ||| amb2_3 2959 ||| sinc7_3 2016 ||| sinc5_3 2016 ||| shar7_3 2016 ||| shar2_3 2016 ||| intel7_3 2016 ||| intel5_3 2016 ||| fun7_3 2016 ||| fun5_3 2016 ||| attr7_3 2016 ||| attr5_3 2016 ||| amb7_3 2016 ||| amb5_3 2016 ||| intel7_2 1984 ||| fun7_2 1984 ||| attr7_2 1984 ||| shar7_2 1974 ||| sinc7_2 1955 ||| amb7_2 1955 ||| expnum 1800 ||| numdat_3 1496 ||| num_in_3 668 |||\n"
     ]
    }
   ],
   "source": [
    "import features_creator as fc #importing feature names made in file features_creator.py\n",
    "fc.count_samples_in_features(data)#count samples for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Features Space of interest (with most samples avalaible)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samerace 8378 ||| order 8378 ||| iid 8378 ||| gender 8378 ||| pid 8368 ||| race 8315 ||| race_o 8305 ||| yoga 8299 ||| tvsports 8299 ||| tv 8299 ||| theater 8299 ||| sports 8299 ||| sinc2_1 8299 ||| sinc1_1 8299 ||| shopping 8299 ||| reading 8299 ||| music 8299 ||| museums 8299 ||| movies 8299 ||| intel2_1 8299 ||| intel1_1 8299 ||| imprelig 8299 ||| imprace 8299 ||| hiking 8299 ||| goal 8299 ||| go_out 8299 ||| gaming 8299 ||| fun2_1 8299 ||| exercise 8299 ||| dining 8299 ||| concerts 8299 ||| clubbing 8299 ||| attr2_1 8299 ||| attr1_1 8299 ||| art 8299 ||| field_cd 8296 ||| shar2_1 8289 ||| pf_o_sin 8289 ||| pf_o_int 8289 ||| pf_o_att 8289 ||| fun1_1 8289 ||| amb2_1 8289 ||| age 8283 ||| date 8281 ||| pf_o_fun 8280 ||| amb1_1 8279 ||| exphappy 8277 ||| age_o 8274 ||| sinc3_1 8273 ||| intel3_1 8273 ||| fun3_1 8273 ||| attr3_1 8273 ||| amb3_1 8273 ||| pf_o_amb 8271 ||| shar1_1 8257 ||| pf_o_sha 8249 ||| career_c 8240 ||| int_corr 8220 ||| attr 8176 ||| attr_o 8166 ||| like 8138 ||| like_o 8128 ||| sinc 8101 ||| sinc_o 8091 ||| intel 8082 ||| intel_o 8072 ||| prob 8069 ||| prob_o 8060 ||| fun 8028 ||| fun_o 8018 ||| met 8003 ||| met_o 7993 ||| amb 7666 ||| amb_o 7656 ||| zipcode 7314 ||| shar 7311 ||| shar_o 7302 |||\n"
     ]
    }
   ],
   "source": [
    "fc.count_samples_in_features(data[fc.feature_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Clean Up: Making Sure Features are within Range</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc.likert_scale_question_3(data)#Scale from 0 - 10. Bring fix rating of 12 to 10\n",
    "fc.scale_question_3(data)#change to scale from 0 - 100, force that features add to 100\n",
    "fc.scale_question_4(data)#change to scale from 0 - 100, force that features add to 100\n",
    "fc.scale_question_5(data)#change to scale from 0 - 100, force that features add to 100\n",
    "fc.scale_question_1(data)#force that features add to 100, if not the case\n",
    "fc.scale_question_2(data)#force that features add to 100, if not the case\n",
    "fc.scale_question_7(data)#force that features add to 100, if not the case\n",
    "fc.scale_rating_received(data)#change to scale from 0 - 100, force that features add to 100\n",
    "fc.scale_rating_given(data)#change to scale from 0 - 100, force that features add to 100\n",
    "fc.scale_half_way(data)#change to scale from 0 - 100, force that features add to 100\n",
    "fc.scale_half_way_2(data)#change to scale from 0 - 100, force that features add to 100\n",
    "fc.scale_age(data)#change to scale from 0 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Type Casting</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc.convert_income_to_float(data)#income was imported as string this call converts strings to float\n",
    "fc.convert_tuition_to_float(data)#ditto, tuition was imported as strings and are converted to float\n",
    "fc.zipcode_to_float(data)#zipcode strings converted to float\n",
    "fc.sat_to_float(data)#this function converts sat scores to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Outlier Detection: Turkey's Method</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8203, 8209, 8219, 8359, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 8204, 720, 739, 8217, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 911, 920, 921, 951, 961, 981, 991, 1001, 8218, 1695, 1715, 1765, 1785, 1795, 1805, 1825, 1835, 1839, 1841, 1866, 1867, 1868, 1869, 1870, 8208, 8271, 8207, 4861, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5032, 5035, 5054, 5074, 5076, 5096, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5138, 5139, 5176, 5246, 5247, 5284, 5295, 5296, 5299, 5302, 5303, 5305, 5306, 5310, 5311, 5320, 5334, 5352, 5355, 5375, 5378, 5388, 5419, 5439, 5481, 5482, 5499, 5510, 5516, 5530, 5536, 5542, 5544, 5552, 5558, 5559, 5560, 5579, 5583, 5604, 5619, 5625, 5639, 5643, 5666, 5689, 5706, 5719, 5739, 5746, 5765, 5784, 5822, 5936, 6031, 6069, 8235, 6122, 6123, 6124, 6126, 6132, 6137, 6139, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 8220, 7420, 7422, 7438, 7446, 7468, 7483, 7485, 7493, 7497, 7508, 7534, 7556, 7578, 7588, 7591, 7594, 7597, 7599, 7642, 7650, 7667, 7754, 8344, 7776, 7798, 7820, 7834, 7835, 7946, 8012, 8027, 8051, 8067, 8078, 8095, 8114, 8118, 8144, 8158, 8188]\n"
     ]
    }
   ],
   "source": [
    "index_to_be_removed = fc.outlier_detection(data[fc.feature_space[10:72]]) \n",
    "#these indices span at least 15  features as outliers\n",
    "print index_to_be_removed\n",
    "data.drop(data.index[index_to_be_removed], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Basic Stats for Unique Females</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fc.dating_attributes_vs_time_describe(data = data, gender = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Frequency Charts for Females</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fc.dating_attributes_vs_time_hist(data = data, gender = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Basic Stats for Unique Males</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fc.dating_attributes_vs_time_describe(data = data, gender = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Frequency Charts for Males</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fc.dating_attributes_vs_time_hist(data = data, gender = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Scale Numerical features between 0 & 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fc.scale_majority_of_features(data)#this function scales most features between 0 - 1\n",
    "fc.scale_exphappy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Correlation Heat Map</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fc.make_corr(data[fc.feature_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Forest Feature Selection: ExtraTreesClassifier & RandomForestClassifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "women_men = data[fc.all_space].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "women_men.dropna(axis = 0, how = 'any', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Both Genders</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_df = women_men['dec'].copy()\n",
    "input_df = women_men[fc.feature_space].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fc.forests(input_df, target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Feature Selection: SelectKBest, F-Classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_classif\\nkBest = SelectKBest(f_classif, k = 'all')\\nkBest.fit_transform(input_df, target_df)\\nk_Best_features = [(j, i, k) for i, j, k in zip(input_df.keys(), kBest.scores_, kBest.pvalues_)]\\nk_Best_features.sort()\\nk_Best_features.reverse()\\ncounter = 0\\nprint 'SelectKBest: f_classif'\\nfor i in k_Best_features:\\n    counter += 1\\n    print counter, i\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "kBest = SelectKBest(f_classif, k = 'all')\n",
    "kBest.fit_transform(input_df, target_df)\n",
    "k_Best_features = [(j, i, k) for i, j, k in zip(input_df.keys(), kBest.scores_, kBest.pvalues_)]\n",
    "k_Best_features.sort()\n",
    "k_Best_features.reverse()\n",
    "counter = 0\n",
    "print 'SelectKBest: f_classif'\n",
    "for i in k_Best_features:\n",
    "    counter += 1\n",
    "    print counter, i\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create Array of Selected Features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'attr', 'intel', 'shar', 'sinc', 'amb', 'fun', 'prob']\n"
     ]
    }
   ],
   "source": [
    "features_selected = ['like', 'attr', 'intel', 'shar', 'sinc', 'amb', 'fun', 'prob']\n",
    "print features_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Feature Selection: Chi2</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn import preprocessing\\nnew_input_df = input_df.copy()\\nnew_input_df['int_corr'] = (new_input_df - new_input_df.min()) / (new_input_df.max() - new_input_df.min())\\nnew_input_df.drop(labels = ['iid', 'gender', 'race', 'field_cd','career_c', 'goal', 'date', 'zipcode', 'imprelig', 'imprace', 'prob_o', 'met', 'go_out', \\n                            'race_o', 'samerace','pid','order', 'met_o'], axis = 1, inplace = True)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn import preprocessing\n",
    "new_input_df = input_df.copy()\n",
    "new_input_df['int_corr'] = (new_input_df - new_input_df.min()) / (new_input_df.max() - new_input_df.min())\n",
    "new_input_df.drop(labels = ['iid', 'gender', 'race', 'field_cd','career_c', 'goal', 'date', 'zipcode', 'imprelig', 'imprace', 'prob_o', 'met', 'go_out', \n",
    "                            'race_o', 'samerace','pid','order', 'met_o'], axis = 1, inplace = True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new_input_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.feature_selection import chi2\\nkBest = SelectKBest(chi2, k = 'all')\\nkBest.fit_transform(new_input_df, target_df)\\nk_Best_features = [(j, i, k) for i, j, k in zip(new_input_df, kBest.scores_, kBest.pvalues_)]\\nk_Best_features.sort()\\nk_Best_features.reverse()\\ncounter = 0\\nprint 'SelectKBest: chi2'\\nfor i in k_Best_features:\\n    counter += 1\\n    print counter, i\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.feature_selection import chi2\n",
    "kBest = SelectKBest(chi2, k = 'all')\n",
    "kBest.fit_transform(new_input_df, target_df)\n",
    "k_Best_features = [(j, i, k) for i, j, k in zip(new_input_df, kBest.scores_, kBest.pvalues_)]\n",
    "k_Best_features.sort()\n",
    "k_Best_features.reverse()\n",
    "counter = 0\n",
    "print 'SelectKBest: chi2'\n",
    "for i in k_Best_features:\n",
    "    counter += 1\n",
    "    print counter, i\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalization features: PCA</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new_input_df = input_df.copy()\\nnew_input_df = preprocessing.normalize(new_input_df[features_selected].copy())\\nfrom sklearn.decomposition import PCA\\ntotal_ratio = []\\nfor i in range(1, 9):\\n    pca = PCA(n_components = i)\\n    pca.fit(new_input_df)\\n    #print 'n = ', str(i), ', ', 'Total Ratio: ', pca.explained_variance_ratio_.sum()\\n    #print 'Components:', pca.components_\\n    #print 'Explained Variance:', pca.explained_variance_ \\n    #print 'Explained Ratio: ', pca.explained_variance_ratio_\\n    total_ratio.append(pca.explained_variance_ratio_.sum())\\n \\nplt.bar(range(1,9), total_ratio, align='center', alpha = 1)\\nplt.xlabel('Projections')\\nplt.ylabel('Total Explained Ratio')\\nplt.title('Total Explained Ratio vs. Projections')\\n \\nplt.show()\\ncounter = 1\\nfor i in total_ratio:\\n    print 'n = ', counter, ', ', i\\n    counter += 1\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"new_input_df = input_df.copy()\n",
    "new_input_df = preprocessing.normalize(new_input_df[features_selected].copy())\n",
    "from sklearn.decomposition import PCA\n",
    "total_ratio = []\n",
    "for i in range(1, 9):\n",
    "    pca = PCA(n_components = i)\n",
    "    pca.fit(new_input_df)\n",
    "    #print 'n = ', str(i), ', ', 'Total Ratio: ', pca.explained_variance_ratio_.sum()\n",
    "    #print 'Components:', pca.components_\n",
    "    #print 'Explained Variance:', pca.explained_variance_ \n",
    "    #print 'Explained Ratio: ', pca.explained_variance_ratio_\n",
    "    total_ratio.append(pca.explained_variance_ratio_.sum())\n",
    " \n",
    "plt.bar(range(1,9), total_ratio, align='center', alpha = 1)\n",
    "plt.xlabel('Projections')\n",
    "plt.ylabel('Total Explained Ratio')\n",
    "plt.title('Total Explained Ratio vs. Projections')\n",
    " \n",
    "plt.show()\n",
    "counter = 1\n",
    "for i in total_ratio:\n",
    "    print 'n = ', counter, ', ', i\n",
    "    counter += 1\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>PCA: Projections to 3 dimensions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from mpl_toolkits.mplot3d import Axes3D\\n\\npca = PCA(n_components = 3)\\npca.fit(X = new_input_df, y = target_df)\\ntransformed_new_input_df = pca.fit_transform(X = new_input_df, y = target_df)\\ntransformed_pca = pd.DataFrame(transformed_new_input_df, columns = ['x_s', 'y_s', 'z_s'])\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(X = new_input_df, y = target_df)\n",
    "transformed_new_input_df = pca.fit_transform(X = new_input_df, y = target_df)\n",
    "transformed_pca = pd.DataFrame(transformed_new_input_df, columns = ['x_s', 'y_s', 'z_s'])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plotting Chosen Features on PCA</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fc.pca_plotter(transformed_pca, target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plotting Chosen Features on ln(PCA)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#log_transformed_pca = np.log(transformed_pca.copy())\n",
    "#fc.pca_plotter(log_transformed_pca, target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plotting Chosen Features on (PCA)^3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cubed_transformed_pca = (transformed_pca.copy())**3\n",
    "#fc.pca_plotter(cubed_transformed_pca, target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Nearest Neighboor Alogorithm: Gridsearch for first 100 neighboors</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.grid_search import GridSearchCV\\nfrom sklearn.metrics import make_scorer\\nfrom sklearn import cross_validation\\nfrom sklearn.metrics import f1_score\\nfrom sklearn import neighbors\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\ntransformed_new_input_df = pca.fit_transform(X = new_input_df, y = target_df)\\ntransformed_pca = pd.DataFrame(transformed_new_input_df, columns = ['x_s', 'y_s', 'z_s'])\\n\\nnum_train = int(.75 * transformed_pca.shape[0])\\nnum_test = int(transformed_pca.shape[0] - num_train)\\n\\nprint 'Training set size: ', num_train, 'Test set size: ', num_test\\n\\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(transformed_pca, target_df, test_size = num_test, random_state = 0)\\n\\nf1_scorer = make_scorer(f1_score, pos_label = 1)\\nparameters = [{'n_neighbors' : range(1,101)}]\\nclf = neighbors.KNeighborsClassifier()\\nclf = GridSearchCV(estimator = clf, param_grid = parameters, scoring = f1_scorer, cv = 10)\\nclf.fit(X_train, y_train)\\nprint 'Best f1_score: ', clf.best_score_ \\nprint 'With neighboors at: ', clf.best_params_\\nprint 'Best Estimator: ', clf.best_estimator_\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "transformed_new_input_df = pca.fit_transform(X = new_input_df, y = target_df)\n",
    "transformed_pca = pd.DataFrame(transformed_new_input_df, columns = ['x_s', 'y_s', 'z_s'])\n",
    "\n",
    "num_train = int(.75 * transformed_pca.shape[0])\n",
    "num_test = int(transformed_pca.shape[0] - num_train)\n",
    "\n",
    "print 'Training set size: ', num_train, 'Test set size: ', num_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(transformed_pca, target_df, test_size = num_test, random_state = 0)\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label = 1)\n",
    "parameters = [{'n_neighbors' : range(1,101)}]\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf = GridSearchCV(estimator = clf, param_grid = parameters, scoring = f1_scorer, cv = 10)\n",
    "clf.fit(X_train, y_train)\n",
    "print 'Best f1_score: ', clf.best_score_ \n",
    "print 'With neighboors at: ', clf.best_params_\n",
    "print 'Best Estimator: ', clf.best_estimator_\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Fit Test Set</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.metrics import accuracy_score, classification_report\\ny_pred = clf.predict(X_test)\\nprint 'Test Size: ', X_test.shape[0]\\nprint 'Accuracy:' , accuracy_score(y_test, y_pred)\\nprint classification_report(y_test, y_pred)\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = clf.predict(X_test)\n",
    "print 'Test Size: ', X_test.shape[0]\n",
    "print 'Accuracy:' , accuracy_score(y_test, y_pred)\n",
    "print classification_report(y_test, y_pred)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Matched People DataFrame</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#people_matched = data[data['match'] == 1].copy()\n",
    "#people_matched.drop_duplicates(subset = 'iid', keep = 'first', inplace = True)\n",
    "#display(people_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploring Matches</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#people_matched[['iid', 'gender', 'dec'] + fc.features_of_attraction + fc.preferences_of_attraction + ['dec_o', 'pid', 'goal', 'int_corr', 'match']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Get Index for 'iid' for non-matches</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number = [int(i) for i in people_matched['iid']]\n",
    "#not_ever_matched = [i for i in range(1,553) if i not in number]\n",
    "#print not_ever_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#people_not_matched = data[data['iid'].isin(not_ever_matched)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploring Non-Matches</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#people_not_matched[['iid', 'gender', 'dec'] + fc.features_of_attraction + fc.preferences_of_attraction + ['dec_o', 'pid', 'goal', 'int_corr', 'match']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Non-Matched Females: Graphs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fc.dating_attributes_vs_time(data = people_not_matched, gender = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Non-Matched Males: Graphs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fc.dating_attributes_vs_time(data = people_not_matched, gender = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#num_train = int(.75 * transformed_pca.shape[0])\n",
    "#num_test = int(transformed_pca.shape[0] - num_train)\n",
    "\n",
    "#print 'Training set size: ', num_train, 'Test set size: ', num_test\n",
    "\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(transformed_pca, target_df, test_size = num_test, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'C': array([  1.00000000e-02,   2.78255940e-02,   7.74263683e-02,\n",
      "         2.15443469e-01,   5.99484250e-01,   1.66810054e+00,\n",
      "         4.64158883e+00,   1.29154967e+01,   3.59381366e+01,\n",
      "         1.00000000e+02])}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1_score:  0.628237647132\n",
      "Best C:  {'C': 35.938136638046259}\n",
      "Best Estimator:  SVC(C=35.938136638046259, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Retrain on Best C\n",
      "Coefficients:  [[ 10.99986788  -6.66868821 -17.3551051 ]]\n",
      "Intercept:  [ 4.71620589]\n",
      "Test Size:  1192\n",
      "Accuracy: 0.679530201342\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.69      0.71       664\n",
      "          1       0.63      0.66      0.65       528\n",
      "\n",
      "avg / total       0.68      0.68      0.68      1192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fc.linear_classifier(input_df[['attr', 'amb', 'intel']], target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
